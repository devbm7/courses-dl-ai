{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3416ed55-4e38-4cdd-afe0-e14aa7d857b4",
   "metadata": {},
   "source": [
    "# L5: Generate Data & Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759c10e6-f528-4431-8676-0317312427c2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import lamini\n",
    "lamini.api_key = \"e650b0aaad19a8ac43752657c882cc3405398636449d41b947d80c399f1d0407\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d077ab9-6562-485a-b5f9-9c518fb70f62",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from typing import AsyncIterator, Iterator, Union\n",
    "import sqlite3\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from lamini.generation.base_prompt_object import PromptObject\n",
    "from lamini.generation.generation_node import GenerationNode\n",
    "from lamini.generation.base_prompt_object import PromptObject\n",
    "from lamini.generation.generation_pipeline import GenerationPipeline\n",
    "from util.get_schema import get_schema, get_schema_s\n",
    "from util.make_llama_3_prompt import make_llama_3_prompt\n",
    "from util.setup_logging import setup_logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "engine = sqlite3.connect(\"./nba_roster.db\")\n",
    "setup_logging()\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, \n",
    "                 max_examples=100, \n",
    "                 sql_model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                 gold_file_name=\"gold-test-set.jsonl\",\n",
    "                 training_file_name=\"generated_queries.jsonl\",\n",
    "                 num_to_generate=10):\n",
    "        self.sql_model_name = sql_model_name\n",
    "        self.max_examples = max_examples\n",
    "        self.gold_file_name = gold_file_name\n",
    "        self.training_file_name = training_file_name\n",
    "        self.num_to_generate = num_to_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d587d9d-01f6-41c6-97b2-8060cc60fa0d",
   "metadata": {},
   "source": [
    "## Working Backwards from what you have:\n",
    "### <font color=\"blue\">First</font>: From Scheme and example, generate <font color=\"blue\">new SQL queries</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d0131f-12bb-4af3-8f0d-f525194e6a3f",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "system += (\n",
    "    \"Consider a table called 'nba_roster' with the following schema (columns)\\n\"\n",
    ")\n",
    "system += get_schema_s()\n",
    "system += \"Consider the following questions, and queries used to answer them:\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2941eea5-3034-4d6c-9ece-c8fd149cb541",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an NBA analyst with 15 years of experience writing complex SQL queries.\\nConsider a table called \\'nba_roster\\' with the following schema (columns)\\n0|Team|TEXT eg. \"Toronto Raptors\"\\n1|NAME|TEXT eg. \"Otto Porter Jr.\"\\n2|Jersey|TEXT eg. \"0\" and when null has a value \"NA\"\\n3|POS|TEXT eg. \"PF\"\\n4|AGE|INT eg. \"22\" in years\\n5|HT|TEXT eg. `6\\' 7\"` or `6\\' 10\"` castable to int\\n6|WT|TEXT eg. \"232 lbs\" \\n7|COLLEGE|TEXT eg. \"Michigan\" and when null has a value \"--\"\\n8|SALARY|TEXT eg. \"$9,945,830\" and when null has a value \"--\"\\nConsider the following questions, and queries used to answer them:\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "028c2eb3-28c2-4892-b52d-454266072393",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "question = \"\"\"What is the median weight in the NBA?\"\"\"\n",
    "sql = \"select CAST(SUBSTR(WT, 1, INSTR(WT,' ')) as INTEGER) as percentile from nba_roster order by percentile limit 1 offset (select count(*) from nba_roster)/2;\"\n",
    "\n",
    "system += \"Question: \" + question + \"\\n\"\n",
    "system += \"Query: \" + sql + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83fcc4c-b1c1-4121-97e9-6c1dd5414b16",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an NBA analyst with 15 years of experience writing complex SQL queries.\n",
      "Consider a table called 'nba_roster' with the following schema (columns)\n",
      "0|Team|TEXT eg. \"Toronto Raptors\"\n",
      "1|NAME|TEXT eg. \"Otto Porter Jr.\"\n",
      "2|Jersey|TEXT eg. \"0\" and when null has a value \"NA\"\n",
      "3|POS|TEXT eg. \"PF\"\n",
      "4|AGE|INT eg. \"22\" in years\n",
      "5|HT|TEXT eg. `6' 7\"` or `6' 10\"` castable to int\n",
      "6|WT|TEXT eg. \"232 lbs\" \n",
      "7|COLLEGE|TEXT eg. \"Michigan\" and when null has a value \"--\"\n",
      "8|SALARY|TEXT eg. \"$9,945,830\" and when null has a value \"--\"\n",
      "Consider the following questions, and queries used to answer them:\n",
      "Question: What is the median weight in the NBA?\n",
      "Query: select CAST(SUBSTR(WT, 1, INSTR(WT,' ')) as INTEGER) as percentile from nba_roster order by percentile limit 1 offset (select count(*) from nba_roster)/2;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6af2fb-00cb-4ee1-8c96-e7bcb42574b3",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "user = \"Write two queries that are similar but different to those above.\\n\"\n",
    "user += \"Format the queries as a JSON object, i.e.\\n\"\n",
    "user += '{ \"explanation\": str, \"sql_query_1\" : str, \"sql_query_2\": str }.\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7879b875-3ae5-4f46-b4d1-6dedaad2385a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write two queries that are similar but different to those above.\n",
      "Format the queries as a JSON object, i.e.\n",
      "{ \"explanation\": str, \"sql_query_1\" : str, \"sql_query_2\": str }.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcb14c",
   "metadata": {},
   "source": [
    "We often have a lot of data, but it might not be in the right format for LLMs, so we can use LLMs to reformat this data easily\n",
    "\n",
    "![image](Data_format.png)\n",
    "\n",
    "**Tips for making a prompt**\n",
    "- Add examples\n",
    "- Generate variations\n",
    "- Filter Generation\n",
    "- Manual Examination of what worked and didn't worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3501c5a-094d-4fd2-9b15-ce7d108b0d43",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "user += \"First write an explanation of why you decided to write these new queries in about 3-5 sentences, then write valid sqlite SQL queries for each of the 2 new queries. Make sure each query is complete and ends with a ;\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b5065e-d4eb-43a7-bd8f-efbc294a7dcc",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write two queries that are similar but different to those above.\n",
      "Format the queries as a JSON object, i.e.\n",
      "{ \"explanation\": str, \"sql_query_1\" : str, \"sql_query_2\": str }.\n",
      "First write an explanation of why you decided to write these new queries in about 3-5 sentences, then write valid sqlite SQL queries for each of the 2 new queries. Make sure each query is complete and ends with a ;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e474f237-4f1d-4b71-a761-23663380c75a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prompt = make_llama_3_prompt(user, system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "980367c9-993d-41af-8667-0bf1da622929",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'I decided to write these new queries to provide more insights into the NBA data. The first query calculates the average height of players in the NBA, while the second query finds the team with the highest average salary. These queries are similar to the original query in that they involve extracting and manipulating', 'sql_query_1': \"SELECT AVG(CAST(SUBSTR(HT, 1, INSTR(HT,'')-1) AS INTEGER)) AS average_height FROM nba_roster\", 'sql_query_2': \"SELECT Team, AVG(CAST(SUBSTR(SALARY, 2, LENGTH(SALARY)-2) AS INTEGER)) AS average_salary FROM nba_roster WHERE SALARY!= '--' GROUP BY Team ORDER BY average_salary DESC LIMIT 1\"}\n"
     ]
    }
   ],
   "source": [
    "llm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "result = llm.generate(prompt, output_type={ \"explanation\": \"str\", \"sql_query_1\" : \"str\", \"sql_query_2\": \"str\" }, max_new_tokens=200)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c63d1e4c-21ec-4c8a-9074-7860695ef15d",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "def check_sql_query(query):\n",
    "    try:\n",
    "        pd.read_sql(query, con=engine)\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error in SQL query: {e}\")\n",
    "        return False\n",
    "\n",
    "    logger.info(f\"SQL query {query} is valid\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "560a20a5-bf9a-4e55-8bc3-424899981032",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sql_query(result[\"sql_query_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8fd8cbe-84ab-4888-a77c-b0a2ab1f24c2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sql_query(result[\"sql_query_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3259e",
   "metadata": {},
   "source": [
    "**Wrap everything in Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2a5f816-b17d-4cfb-b706-ea008c26532f",
   "metadata": {
    "height": 1560
   },
   "outputs": [],
   "source": [
    "class ModelStage(GenerationNode):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        prompt = self.add_template(prompt)\n",
    "\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\n",
    "                \"explanation\": \"str\",\n",
    "                \"sql_query_1\": \"str\",\n",
    "                \"sql_query_2\": \"str\",\n",
    "            },\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    async def add_template(self, prompts):\n",
    "        async for prompt in prompts:\n",
    "            new_prompt = make_llama_3_prompt(**self.make_prompt(prompt.data))\n",
    "            yield PromptObject(prompt=new_prompt, data=prompt.data)\n",
    "\n",
    "    async def process_results(self, results):\n",
    "        async for result in results:\n",
    "            if result is None:\n",
    "                continue\n",
    "\n",
    "            if result.response is None:\n",
    "                continue\n",
    "\n",
    "            logger.info(\"=====================================\")\n",
    "            logger.info(f\"Generated query 1: {result.response['sql_query_1']}\")\n",
    "            logger.info(f\"Generated query 2: {result.response['sql_query_2']}\")\n",
    "            logger.info(\"=====================================\")\n",
    "\n",
    "            if self.check_sql_query(result.response[\"sql_query_1\"]):\n",
    "                new_result = PromptObject(prompt=\"\", data=copy.deepcopy(result.data))\n",
    "                new_result.data.generated_sql_query = result.response[\"sql_query_1\"]\n",
    "                yield new_result\n",
    "\n",
    "            if self.check_sql_query(result.response[\"sql_query_2\"]):\n",
    "                new_result = PromptObject(prompt=\"\", data=copy.deepcopy(result.data))\n",
    "                new_result.data.generated_sql_query = result.response[\"sql_query_2\"]\n",
    "                yield new_result\n",
    "\n",
    "    def make_prompt(self, data):\n",
    "        system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += (\n",
    "            \"Consider a table called 'nba_roster' with the following schema (columns)\\n\"\n",
    "        )\n",
    "        system += get_schema()\n",
    "        system += \"Consider the following questions, and queries used to answer them:\\n\"\n",
    "        for example in data.sample:\n",
    "            system += \"Question: \" + example[\"question\"] + \"\\n\"\n",
    "            system += \"Query: \" + example[\"sql\"] + \"\\n\"\n",
    "\n",
    "        # Important: generate relevant queries to your reference data\n",
    "        # Ideally, close to those that are failing so you can show the model examples of how to do it right!\n",
    "        user = \"Write two queries that are similar but different to those above.\\n\"\n",
    "        user += \"Format the queries as a JSON object, i.e.\\n\"\n",
    "        user += '{ \"explanation\": str, \"sql_query_1\" : str, \"sql_query_2\": str }.\\n'\n",
    "\n",
    "        # Next, use Chain of Thought (CoT) and prompt-engineering to help with generating SQL queries\n",
    "        user += \"First write an explanation of why you decided to write these new queries in about 3-5 sentences, then write valid sqlite SQL queries for each of the 2 new queries. Make sure each query is complete and ends with a ;\\n\"\n",
    "\n",
    "        return {\"system\": system, \"user\": user}\n",
    "\n",
    "    def check_sql_query(self, query):\n",
    "        try:\n",
    "            pd.read_sql(query, con=engine)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error in SQL query: {e}\")\n",
    "            return False\n",
    "\n",
    "        logger.info(f\"SQL query {query} is valid\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21a538-4aba-4901-aed0-11721f475bf4",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Second:</font> Now that you have queries, <font color=\"blue\">generate questions</font> for those queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3ecba8f-4919-4b13-b053-02e7c35fcf24",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "system += (\n",
    "    \"Consider a table called 'nba_roster' with the following schema (columns)\\n\"\n",
    ")\n",
    "system += get_schema() + \"\\n\"\n",
    "system += \"Queries, and questions that they are used to answer:\\n\"\n",
    "\n",
    "example_question = \"\"\"What is the median weight in the NBA?\"\"\"\n",
    "example_sql = \"select CAST(SUBSTR(WT, 1, INSTR(WT,' ')) as INTEGER) as percentile from nba_roster order by percentile limit 1 offset (select count(*) from nba_roster)/2;\"\n",
    "\n",
    "system += \"Question: \" + example_question + \"\\n\"\n",
    "system += \"Query: \" + example_sql + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49952fa4-8690-4780-8b10-b0fc97710733",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "generated_sql = result[\"sql_query_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15766a60",
   "metadata": {},
   "source": [
    "### Generate a quesion for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c55d269-46bd-4650-ad08-5050cfdaa954",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "user = \"Now consider the following query.\\n\"\n",
    "user += \"Query: \" + generated_sql + \"\\n\"\n",
    "user += \"Write a question that this query could be used to answer.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "407f1212-b95d-46a1-8673-ea8fb8260a75",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "user += \"Format your response as a JSON object, i.e.\\n\"\n",
    "user += '{ \"explanation\": str, \"question\": str }.\\n'\n",
    "\n",
    "user += \"First write an explanation in about 3-5 sentences, then write a one sentence question.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8498e552-e3b8-466c-b86f-e73adca0541c",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'This query calculates the average salary for each team in the NBA, excluding teams with unknown salaries. It does this by first removing the dollar sign and any leading or trailing characters from the salary string, then converting the remaining characters to an integer. The results are then grouped by team and ordered in descending order by average salary, with the team having the highest average salary returned first. The LIMIT 1 clause ensures that only the top team is returned', 'question': 'Which team has the highest average salary among all teams in the NBA'}\n"
     ]
    }
   ],
   "source": [
    "prompt = make_llama_3_prompt(user, system)\n",
    "result = llm.generate(prompt, output_type={ \"explanation\": \"str\", \"question\" : \"str\" }, max_new_tokens=200)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5effb87",
   "metadata": {},
   "source": [
    "**Wrap everything in a class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f111edf8-a69f-45e0-b67a-6891827f396d",
   "metadata": {
    "height": 965
   },
   "outputs": [],
   "source": [
    "# Wrap it all up together in a class which generates a question\n",
    "# given a query\n",
    "\n",
    "class QuestionStage(GenerationNode):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=150,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\n",
    "                \"explanation\": \"str\",\n",
    "                \"question\": \"str\",\n",
    "            },\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        new_prompt = make_llama_3_prompt(**self.make_question_prompt(obj.data))\n",
    "        obj.prompt = new_prompt\n",
    "\n",
    "    def make_question_prompt(self, data):\n",
    "        system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += (\n",
    "            \"Consider a table called 'nba_roster' with the following schema (columns)\\n\"\n",
    "        )\n",
    "        system += get_schema() + \"\\n\"\n",
    "        system += \"Queries, and questions that they are used to answer:\\n\"\n",
    "        for example in data.sample:\n",
    "            system += \"Query: \" + example[\"sql\"] + \"\\n\"\n",
    "            system += \"Question: \" + example[\"question\"] + \"\\n\"\n",
    "\n",
    "        user = \"Now consider the following query.\\n\"\n",
    "        user += \"Query: \" + data.generated_sql_query + \"\\n\"\n",
    "        user += \"Write a question that this query could be used to answer.\\n\"\n",
    "\n",
    "        # Using Chain of Thought (CoT) again\n",
    "        # This time you can do it programmatically with function calling, so you can easily extract a question out of the JSON object\n",
    "        user += \"Format your response as a JSON object, i.e.\\n\"\n",
    "        user += '{ \"explanation\": str, \"question\": str }.\\n'\n",
    "\n",
    "        user += \"First write an explanation in about 3-5 sentences, then write a one sentence question.\\n\"\n",
    "\n",
    "        return {\"system\": system, \"user\": user}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab483e",
   "metadata": {},
   "source": [
    "### Create a Pipeline for entire data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fa638ca-ae31-477c-9583-ba59fedcb829",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "class QueryGenPipeline(GenerationPipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_stage = ModelStage()\n",
    "        self.question_stage = QuestionStage()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_stage(x)\n",
    "        x = self.question_stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e6c36c4-60ba-4068-93f4-cc5333d88e35",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "async def run_query_gen_pipeline(gold_queries):\n",
    "    return QueryGenPipeline().call(gold_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa67c68",
   "metadata": {},
   "source": [
    "### Generate N samples, for every example in the gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6314b56a-b4b1-4247-806e-9c6332614a02",
   "metadata": {
    "height": 455
   },
   "outputs": [],
   "source": [
    "\n",
    "all_examples = []\n",
    "\n",
    "async def load_gold_queries(args):\n",
    "    path = f\"data/{args.gold_file_name}\"\n",
    "\n",
    "    with jsonlines.open(path) as reader:\n",
    "        global all_examples\n",
    "\n",
    "        all_examples = [obj for obj in reader]\n",
    "\n",
    "    sample_count = args.num_to_generate\n",
    "    sample_size = 3\n",
    "\n",
    "    random.seed(42)\n",
    "\n",
    "    for i in range(sample_count):\n",
    "        example_sample = ExampleSample(random.sample(all_examples, sample_size), i)\n",
    "        yield PromptObject(prompt=\"\", data=example_sample)\n",
    "\n",
    "\n",
    "class ExampleSample:\n",
    "    def __init__(self, sample, index):\n",
    "        self.sample = sample\n",
    "        self.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5536b611-a63f-4a49-8303-3c70e6de18db",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "async def save_generation_results(results, args):\n",
    "    path = f\"data/training_data/{args.training_file_name}\"\n",
    "\n",
    "    pbar = tqdm(desc=\"Saving results\", unit=\" results\")\n",
    "    with jsonlines.open(path, \"w\") as writer:\n",
    "\n",
    "        async for result in results:\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.response[\"question\"],\n",
    "                    \"sql\": result.data.generated_sql_query,\n",
    "                }\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "        for example in all_examples:\n",
    "            writer.write(example)\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a585d974-0601-4ab3-a289-66059c118d82",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving results: 30 results [00:11,  2.71 results/s]\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "gold_queries = load_gold_queries(args)\n",
    "results = await run_query_gen_pipeline(gold_queries)\n",
    "await save_generation_results(results, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b380c-78e9-4560-9255-b01e8455885d",
   "metadata": {},
   "source": [
    "### Round of finetuning\n",
    "Now that you have data, even if it is not perfect, go through a round of finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e051553d-7c78-4580-a9d4-2ac4dafd1c12",
   "metadata": {
    "height": 625
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from typing import AsyncIterator, Iterator, Union\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from lamini.generation.base_prompt_object import PromptObject\n",
    "from lamini.generation.generation_node import GenerationNode\n",
    "from lamini.generation.base_prompt_object import PromptObject\n",
    "from lamini.generation.generation_pipeline import GenerationPipeline\n",
    "from util.get_schema import get_schema\n",
    "from util.make_llama_3_prompt import make_llama_3_prompt\n",
    "from util.setup_logging import setup_logging\n",
    "from util.load_dataset import get_dataset\n",
    "from util.get_default_finetune_args import get_default_finetune_args\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "engine = sqlite3.connect(\"./nba_roster.db\")\n",
    "setup_logging()\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, \n",
    "                 max_examples=100, \n",
    "                 sql_model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                 gold_file_name=\"gold-test-set.jsonl\",\n",
    "                 training_file_name=\"archive/generated_queries.jsonl\",\n",
    "                 num_to_generate=10):\n",
    "        self.sql_model_name = sql_model_name\n",
    "        self.max_examples = max_examples\n",
    "        self.gold_file_name = gold_file_name\n",
    "        self.training_file_name = training_file_name\n",
    "        self.num_to_generate = num_to_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e4f95-e115-4b2c-8644-b2c0f9032ed4",
   "metadata": {},
   "source": [
    "make_question will take the questions and queries from the training_file and embed them in the prompt below to form the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa9a3cc0-6ff3-4037-8913-eb373e0e1249",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "def make_question(obj):\n",
    "    system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "    system += \"Consider the nba_roster table with the following schema:\\n\"\n",
    "    system += get_schema() + \"\\n\"\n",
    "    system += (\n",
    "        \"Write a sqlite SQL query that would help you answer the following question:\\n\"\n",
    "    )\n",
    "    user = obj[\"question\"]\n",
    "    return {\"system\": system, \"user\": user}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3e2d58b-aaec-4fc4-8435-148fc4b28093",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "args = Args()\n",
    "llm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9cba156-4b6b-499b-8917-28e3b0d9117f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset(args, make_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08205a44-2b1d-417a-9748-d5b5754ac3b9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "finetune_args = get_default_finetune_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9764f-3a48-478e-9ff7-1c9b2ef5f854",
   "metadata": {},
   "source": [
    "This fine tuning step takes about 30 mintues to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "365bca3a-f63c-40f9-b872-7811d8cb6323",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pairs uploaded to local.\n",
      "\n",
      "Your dataset id is: d7b6d89952d7ad0031e74578366c085585b03adacc5a24ec4b41ab60c7083b5f . Consider using this in the future to train using the same data. \n",
      "Eg: llm.train(data_or_dataset_id='d7b6d89952d7ad0031e74578366c085585b03adacc5a24ec4b41ab60c7083b5f')\n",
      "Tuning job submitted! Check status of job 15515 here: https://api.lamini.ai/train/15515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'job_id': 15515,\n",
       " 'status': 'CREATED',\n",
       " 'dataset_id': 'd7b6d89952d7ad0031e74578366c085585b03adacc5a24ec4b41ab60c7083b5f'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.train(\n",
    "   data_or_dataset_id=dataset,\n",
    "   finetune_args=finetune_args,\n",
    "   is_public=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74fb8d-7323-4b84-9da5-72c62ce159ee",
   "metadata": {},
   "source": [
    "We can examine this pre-computed finetuning result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5c75f6d-98ce-4b2d-a888-2b8a7f24ce10",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm = lamini.Lamini(model_name=\"ba84d30365bf555fe2746e0fdd0e3f21689d68e47f5ff9ad923543a31e3a1567\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28440158-0f30-4372-9ee4-ebea8786382e",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " Who is the highest paid NBA player?\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Who is the highest paid NBA player?\"\"\"\n",
    "system = f\"\"\"You are an NBA analyst with 15 years of experience writing complex SQL queries. Consider the nba_roster table with the following schema:\n",
    "{get_schema()}\n",
    "\n",
    "Write a sqlite query to answer the following question. Follow instructions exactly\"\"\"\n",
    "prompt = make_llama_3_prompt(question, system)\n",
    "print(\"Question:\\n\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9be7072-ddc5-4867-a507-09410d955d1e",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "select salary, name from nba_roster where SALARY!= '--' ORDER BY CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER) DESC LIMIT 1 OFFSET (SELECT COUNT(*) FROM nba_roster WHERE SALARY!= '--')*75/100-1 OFFSET)\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\")\n",
    "print(llm.generate(prompt, max_new_tokens=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51b28fdd-24fb-4cd5-bb02-7bed9683596b",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SALARY           NAME\n",
      "0  $51,915,615  Stephen Curry\n"
     ]
    }
   ],
   "source": [
    "query=\"SELECT salary, name FROM nba_roster WHERE salary != '--' ORDER BY CAST(REPLACE(REPLACE(salary, '$', ''), ',','') AS INTEGER) DESC LIMIT 1;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591a200-1c34-4f75-a464-fb8eae73b151",
   "metadata": {},
   "source": [
    "Now lets run an evaluation over the eval dataset. Load code from lesson 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82562369-f5a7-48ae-b67f-89f63aba33dd",
   "metadata": {
    "height": 4161
   },
   "outputs": [],
   "source": [
    "class QueryStage(GenerationNode):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"sqlite_query\": \"str\"},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        query_succeeded = False\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Running SQL query '{obj.response['sqlite_query']}'\")\n",
    "            obj.data[\"generated_query\"] = obj.response[\"sqlite_query\"]\n",
    "            df = pd.read_sql(obj.response[\"sqlite_query\"], con=engine)\n",
    "            obj.data['df'] = df\n",
    "            logger.info(f\"Got data: {df}\")\n",
    "            query_succeeded = True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Failed to run SQL query: {obj.response['sqlite_query']}\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Running reference SQL query '{obj.data['sql']}'\")\n",
    "        df = pd.read_sql(obj.data[\"sql\"], con=engine)\n",
    "        logger.info(f\"Got data: {df}\")\n",
    "        obj.data['reference_df'] = df\n",
    "\n",
    "        logger.info(f\"For question: {obj.data['question']}\")\n",
    "        logger.info(f\"For query: {obj.response['sqlite_query']}\")\n",
    "\n",
    "        obj.data[\"query_succeeded\"] = query_succeeded\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        new_prompt = make_llama_3_prompt(**self.make_prompt(obj.data))\n",
    "        obj.prompt = new_prompt\n",
    "\n",
    "    def make_prompt(self, data: dict):\n",
    "        system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += \"Consider the nba_roster table with the following schema:\\n\"\n",
    "        system += get_schema() + \"\\n\"\n",
    "        system += (\n",
    "            \"Write a sqlite SQL query that would help you answer the following question. Make sure each query ends with a semicolon:\\n\"\n",
    "        )\n",
    "        user = data[\"question\"]\n",
    "        return {\n",
    "            \"user\": user,\n",
    "            \"system\": system,\n",
    "        }\n",
    "    \n",
    "class ScoreStage(GenerationNode):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=150,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"explanation\": \"str\", \"similar\": [\"true\", \"false\"]},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        obj.prompt = make_llama_3_prompt(**self.make_prompt(obj))\n",
    "        logger.info(f\"Scoring Stage Prompt:\\n{obj.prompt}\")\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        obj.data['is_matching'] = self.is_matching(obj.data, obj.response)\n",
    "        obj.data['explanation'] = obj.response[\"explanation\"]\n",
    "        obj.data['similar'] = obj.response[\"similar\"] == \"true\"\n",
    "\n",
    "    def is_matching(self, data, response):\n",
    "        return (str(data.get('df',\"None\")).lower() == str(data['reference_df']).lower() \n",
    "                or response['similar'] == \"true\")\n",
    "\n",
    "    def make_prompt(self, obj: PromptObject):\n",
    "        # Your evaluation model compares SQL output from the generated and reference SQL queries, using another LLM in the pipeline\n",
    "        '''\n",
    "        Note:\n",
    "        Prompt tuning is important! \n",
    "        A previous iteration of this scoring pipeline said `Compare the following two dataframes to see if they are identical`.\n",
    "        That prompt turned out to be too stringent of criteria.\n",
    "        '''\n",
    "        system_prompt = \"Compare the following two dataframes. They are similar if they are almost identical, or if they convey the same information about the nba_roster dataset\"\n",
    "        system_prompt += \"Respond with valid JSON {'explanation' : str, 'similar' : bool}\"\n",
    "        user_prompt = (\n",
    "            f\"========== Dataframe 1 =========\\n{str(obj.data.get('df','None')).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += (\n",
    "            f\"========== Dataframe 2 =========\\n{str(obj.data['reference_df']).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += f\"Can you tell me if these dataframes are similar?\"\n",
    "        return {\n",
    "            \"system\": system_prompt,\n",
    "            \"user\": user_prompt\n",
    "        }\n",
    "    \n",
    "async def run_eval(dataset, args):\n",
    "\n",
    "    results = await run_evaluation_pipeline(dataset, args)\n",
    "\n",
    "    print(\"Total results:\", len(results))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "async def run_evaluation_pipeline(dataset, args):\n",
    "    results = EvaluationPipeline(args).call(dataset)\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    pbar = tqdm(desc=\"Saving results\", unit=\" results\")\n",
    "    async for result in results:\n",
    "        result_list.append(result)\n",
    "        pbar.update()\n",
    "    return result_list\n",
    "\n",
    "\n",
    "class EvaluationPipeline(GenerationPipeline):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.query_stage = QueryStage(args.sql_model_name)\n",
    "        self.score_stage = ScoreStage()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.query_stage(x)\n",
    "        x = self.score_stage(x)\n",
    "        return x\n",
    "    \n",
    "def load_gold_dataset(args):\n",
    "    path = f\"data/{args.gold_file_name}\"\n",
    "\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for index, obj in enumerate(reversed(list(reader))):\n",
    "            if index >= args.max_examples:\n",
    "                break\n",
    "            yield PromptObject(prompt=\"\", data=obj)\n",
    "\n",
    "def save_eval_results(results, args):\n",
    "    base_path = \"./data/results\"\n",
    "    now = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    experiment_name = f\"nba_sql_pipeline_{now}\"\n",
    "    experiment_dir = os.path.join(base_path, experiment_name)\n",
    "    os.makedirs(os.path.join(base_path, experiment_name))\n",
    "\n",
    "    # Write args to file\n",
    "    args_file_name = f\"{experiment_dir}/args.txt\"\n",
    "    with open(args_file_name, \"w\") as writer:\n",
    "        pprint(args.__dict__, writer)\n",
    "\n",
    "\n",
    "    def is_correct(r):\n",
    "        if (\n",
    "            (result.data[\"query_succeeded\"] and result.data['is_matching']) or \n",
    "            result.data[\"generated_query\"] == result.data['sql']\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Write sql results and errors to file\n",
    "    results_file_name = f\"{experiment_dir}/sql_results.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if not is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"reference_sql\": result.data['sql'],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    results_file_name = f\"{experiment_dir}/sql_errors.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Write statistics to file\n",
    "    average_sql_succeeded = sum(\n",
    "        [result.data[\"query_succeeded\"] for result in results]\n",
    "    ) / len(results)\n",
    "    average_correct = sum(\n",
    "        [result.data[\"query_succeeded\"] and result.data['is_matching'] for result in results]\n",
    "    ) / len(results)\n",
    "\n",
    "    file_name = f\"{experiment_dir}/summary.txt\"\n",
    "    with open(file_name, \"w\") as writer:\n",
    "        print(f\"Total size of eval dataset: {len(results)}\", file=writer)\n",
    "        print(f\"Total size of eval dataset: {len(results)}\")\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\", file=writer)\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\")\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\", file=writer)\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31bfbe-8666-465d-b329-510fb229cd50",
   "metadata": {},
   "source": [
    "Run the evaluation and you can see there is more valid SQL and correct queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42dcbbe5-d803-449d-b662-c832e4838dcd",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 21:04:38,168 [ERROR] Failed to run SQL query: SELECT team, AVG(AGE) AS average_age FROM nba_roster GROUP BY team ORDER BY average_age DESC LIMIT 5 OFFSET (SELECT COUNT(*) FROM nba_roster) - 5*25 OFFSET 0\n",
      "2025-01-16 21:04:38,173 [ERROR] Failed to run SQL query: SELECT CAST(AGE as INTEGER) as percentile FROM nba_roster WHERE team='Miami Heat' order by percentile limit 1 offset (select count(*) from nba_roster where team='Miami Heat')*100/2-1)\n",
      "2025-01-16 21:04:38,175 [ERROR] Failed to run SQL query: SELECT CAST(AGE as INTEGER) as percentile FROM nba_roster WHERE AGE IS NOT NULL ORDER BY percentile LIMIT 1 OFFSET (SELECT COUNT(*) FROM nba_roster WHERE AGE IS NOT NULL)*50/100-1)\n",
      "2025-01-16 21:04:39,044 [ERROR] Failed to run SQL query: SELECT salary, NAME FROM nba_roster WHERE team='Dallas Mavericks' AND POS='C' AND SALARY!= '--' ORDER BY CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER) DESC LIMIT 1 OFFSET (SELECT COUNT(*) FROM nba_roster WHERE team='Dallas Mavericks' AND POS='C' AND SALARY!= '--')*75/100-1 OFFSET)\n",
      "2025-01-16 21:04:39,047 [ERROR] Failed to run SQL query: select salary, name from nba_roster where team='Cleveland Cavaliers' and SALARY!= '--' ORDER BY CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER) DESC LIMIT 1 OFFSET (select count(*) from nba_roster where team='Cleveland Cavaliers' and SALARY!= '--')*75/100-1 OFFSET)\n",
      "2025-01-16 21:04:39,050 [ERROR] Failed to run SQL query: select salary, name from nba_roster where team='Memphis Grizzlies' and SALARY!= '--' ORDER BY CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER) DESC LIMIT 1 OFFSET (SELECT COUNT(*) FROM nba_roster WHERE team='Memphis Grizzlies' AND SALARY!= '--')*75/100-1 OFFSET)\n",
      "2025-01-16 21:04:40,588 [ERROR] Failed to run SQL query: SELECT AVG(CAST(SUBSTR(HT, 1, INSTR(HT,' ')-1) AS INTEGER)+ CAST(SUBSTR(HT, INSTR(HT,' ')+1) AS FLOAT)/12 AS height FROM nba_roster;\n",
      "2025-01-16 21:04:40,591 [ERROR] Failed to run SQL query: select CAST(SUBSTR(HT, 1, INSTR(HT,' ')-1) AS INTEGER)+ CAST(SUBSTR(HT, INSTR(HT,' ')+1) AS FLOAT)/12 as percentile from nba_roster order by percentile limit 1 offset (select count(*) from nba_roster)*50/100-1)\n",
      "2025-01-16 21:04:41,541 [ERROR] Failed to run SQL query: SELECT (CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER) as percentile FROM nba_roster WHERE SALARY!= '--' order by percentile limit 1 offset (select count(*) from nba_roster where SALARY!= '--')*25/100-1)\n",
      "2025-01-16 21:04:41,544 [ERROR] Failed to run SQL query: SELECT (CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER) as percentile FROM nba_roster WHERE SALARY!= '--' order by percentile limit 1 offset (select count(*) from nba_roster where SALARY!= '--')*75/100-1)\n",
      "2025-01-16 21:04:41,547 [ERROR] Failed to run SQL query: SELECT (CAST(REPLACE(REPLACE(SALARY, '$', ''), ',','') AS INTEGER) as percentile FROM nba_roster WHERE SALARY!= '--' order by percentile limit 1 offset (select count(*) from nba_roster where SALARY!= '--')*99/100-1)\n",
      "Saving results: 20 results [00:55,  2.78s/ results]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 20\n",
      "Total size of eval dataset: 20\n",
      "Percent Valid SQL Syntax: 45.0\n",
      "Percent Correct SQL Query: 35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = Args(sql_model_name=\"ba84d30365bf555fe2746e0fdd0e3f21689d68e47f5ff9ad923543a31e3a1567\")\n",
    "dataset = load_gold_dataset(args)\n",
    "results = await run_eval(dataset, args)\n",
    "save_eval_results(results, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001144e-14b0-4573-a61f-ca8af167aafe",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69b695-afa2-41bc-9252-b62609371843",
   "metadata": {},
   "source": [
    "### Filtering the dataset\n",
    "Next step is filtering. Manually create functions to filter the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d05f25e-a101-4718-99a9-0701c511869c",
   "metadata": {
    "height": 829
   },
   "outputs": [],
   "source": [
    "question_set = set()\n",
    "sql_set = set()\n",
    "\n",
    "def is_not_valid_sql(question, sql):\n",
    "    try:\n",
    "        df = pd.read_sql(sql, con=engine)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        return True\n",
    "\n",
    "def has_null_in_sql_or_question(question, sql):\n",
    "    return \"null\" in sql.lower() or \"null\" in question\n",
    "\n",
    "def returns_empty_dataframe(question, sql):\n",
    "    try:\n",
    "        df = pd.read_sql(sql, con=engine)\n",
    "        return \"Empty\" in str(df) or \"None\" in str(df)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "def uses_avg_on_ht_column(question, sql):\n",
    "    return \"avg(ht)\" in sql.lower() or \"avg(salary\" in sql.lower() \n",
    "\n",
    "filter_conditions = [is_not_valid_sql, has_null_in_sql_or_question, returns_empty_dataframe, uses_avg_on_ht_column]\n",
    "\n",
    "def training_semicolon(sql):\n",
    "    if sql.strip()[-1] != \";\":\n",
    "        return sql.strip() + \";\"\n",
    "    return sql\n",
    "\n",
    "with jsonlines.open(\"data/training_data/archive/generated_queries.jsonl\", \"r\") as reader:\n",
    "    with jsonlines.open(\"data/training_data/generated_queries_filtered.jsonl\", \"w\") as writer:\n",
    "        for r in reader:\n",
    "            if r[\"question\"] in question_set or r[\"sql\"] in sql_set:\n",
    "                continue\n",
    "            question_set.add(r[\"question\"])\n",
    "            sql_set.add(r[\"sql\"])\n",
    "            \n",
    "            if any(c(r['question'], r['sql']) for c in filter_conditions):\n",
    "                continue\n",
    "\n",
    "            sql = training_semicolon(r['sql'])\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": r[\"question\"],\n",
    "                    \"sql\": sql,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614fb03-72ac-4d4b-b256-e9f884aff165",
   "metadata": {},
   "source": [
    "> The following cell is expected to create an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "415acb2d-efc4-4ea1-a35d-ff8d4029db12",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT AVG(CAST(SUBSTR(WT, 1, INSTR(WT,' ')) as INTEGER) FROM nba_roster WHERE WT!= 'NA') as median': near \"FROM\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: near \"FROM\": syntax error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT AVG(CAST(SUBSTR(WT, 1, INSTR(WT,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)) as INTEGER) FROM nba_roster WHERE WT!= \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) as median\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT AVG(CAST(SUBSTR(WT, 1, INSTR(WT,' ')) as INTEGER) FROM nba_roster WHERE WT!= 'NA') as median': near \"FROM\": syntax error"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT AVG(CAST(SUBSTR(WT, 1, INSTR(WT,' ')) as INTEGER) FROM nba_roster WHERE WT!= 'NA') as median\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "943d3442-f951-4975-a3b9-3d520b4b6c32",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    COLLEGE  count\n",
      "0  Kentucky     28\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT COLLEGE, COUNT(*) as count FROM nba_roster WHERE COLLEGE!= '--' GROUP BY COLLEGE ORDER BY count DESC LIMIT 1\", con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "38dfdbc7-ee76-4e43-9a46-efddfd055aa5",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Model tuned on `archive/generated_queries_large_filtered_cleaned.jsonl`\n",
    "llm = lamini.Lamini(model_name=\"63fd73a775daf24216b46c680a1e963a8d1e02b21bca43fcea6c26737d2e887e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc704335-2c00-4895-a6c9-66b540a49189",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " What is the median age of the Chicago Bulls?\n",
      "Answer:\n",
      "select CAST(AGE as INTEGER) as percentile from nba_roster where team='Chicago Bulls' order by percentile limit 1 offset (select count(*) from nba_roster where team='Chicago Bulls')/2;\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"What is the median age of the Chicago Bulls?\"\"\"\n",
    "system = f\"\"\"You are an NBA analyst with 15 years of experience writing complex SQL queries. Consider the nba_roster table with the following schema:\n",
    "{get_schema()}\n",
    "\n",
    "Write a sqlite query to answer the following question. Follow instructions exactly\"\"\"\n",
    "prompt = make_llama_3_prompt(question, system)\n",
    "print(\"Question:\\n\", question)\n",
    "\n",
    "print(\"Answer:\")\n",
    "sql = llm.generate(prompt, max_new_tokens=200)\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2a3f41c-ccde-477b-b59f-48abcf219331",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   percentile\n",
      "0          25\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(sql, con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecd833ae-7bac-41da-8be3-520abef20cc3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "args = Args(training_file_name=\"archive/generated_queries.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e801e164-deac-4c43-b0d6-a7fc6e9b28ed",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "946d6519-c2cf-49d3-96a0-8e428e5266f7",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset(args, make_question)\n",
    "finetune_args = get_default_finetune_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390f9d1-e4d6-4044-b4d0-a0ad40e601ab",
   "metadata": {},
   "source": [
    "This fine tuning step takes about 30 mintues to complete. The dispatch to run on the Lamini services is commented out and the pre-computed final results of the run are provided below. You can uncomment and run if you have modified data on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecb9f2-23d4-4990-8b4c-edf4e6a63e0b",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "#llm.train(\n",
    "#    data_or_dataset_id=dataset,\n",
    "#    finetune_args=finetune_args,\n",
    "#    is_public=True,  # For sharing\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b48a7-0639-47b9-89f3-cbeab1d8bee8",
   "metadata": {},
   "source": [
    "Run eval platform again from lab 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a5c7493-0046-4d9c-bde9-ca48520091d1",
   "metadata": {
    "height": 4161
   },
   "outputs": [],
   "source": [
    "# Collapsible or utils from Lesson 3 Lab for evaluation\n",
    "class QueryStage(GenerationNode):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"sqlite_query\": \"str\"},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        # Run both the generated and reference (Gold Dataset) SQL queries\n",
    "        # Assessing whether the SQL queries succeeded in hitting the database (not correctness yet!)\n",
    "        \n",
    "        query_succeeded = False\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Running SQL query '{obj.response['sqlite_query']}'\")\n",
    "            obj.data[\"generated_query\"] = obj.response[\"sqlite_query\"]\n",
    "            df = pd.read_sql(obj.response[\"sqlite_query\"], con=engine)\n",
    "            obj.data['df'] = df\n",
    "            logger.info(f\"Got data: {df}\")\n",
    "            query_succeeded = True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Failed to run SQL query: {obj.response['sqlite_query']}\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Running reference SQL query '{obj.data['sql']}'\")\n",
    "        df = pd.read_sql(obj.data[\"sql\"], con=engine)\n",
    "        logger.info(f\"Got data: {df}\")\n",
    "        obj.data['reference_df'] = df\n",
    "\n",
    "        logger.info(f\"For question: {obj.data['question']}\")\n",
    "        logger.info(f\"For query: {obj.response['sqlite_query']}\")\n",
    "\n",
    "        obj.data[\"query_succeeded\"] = query_succeeded\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        new_prompt = make_llama_3_prompt(**self.make_prompt(obj.data))\n",
    "        obj.prompt = new_prompt\n",
    "\n",
    "    def make_prompt(self, data: dict):\n",
    "        system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += \"Consider the nba_roster table with the following schema:\\n\"\n",
    "        system += get_schema() + \"\\n\"\n",
    "        system += (\n",
    "            \"Write a sqlite SQL query that would help you answer the following question:\\n\"#\"Write a sqlite SQL query that would help you answer the following question:\\n\"\n",
    "        )\n",
    "        user = data[\"question\"]\n",
    "        return {\n",
    "            \"user\": user,\n",
    "            \"system\": system,\n",
    "        }\n",
    "    \n",
    "class ScoreStage(GenerationNode):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=150,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"explanation\": \"str\", \"similar\": [\"true\", \"false\"]},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        obj.prompt = make_llama_3_prompt(**self.make_prompt(obj))\n",
    "        logger.info(f\"Scoring Stage Prompt:\\n{obj.prompt}\")\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        obj.data['is_matching'] = self.is_matching(obj.data, obj.response)\n",
    "        obj.data['explanation'] = obj.response[\"explanation\"]\n",
    "        obj.data['similar'] = obj.response[\"similar\"] == \"true\"\n",
    "\n",
    "    def is_matching(self, data, response):\n",
    "        return (str(data.get('df',\"None\")).lower() == str(data['reference_df']).lower() \n",
    "                or response['similar'] == \"true\")\n",
    "\n",
    "    def make_prompt(self, obj: PromptObject):\n",
    "        # Your evaluation model compares SQL output from the generated and reference SQL queries, using another LLM in the pipeline\n",
    "        '''\n",
    "        Note:\n",
    "        Prompt tuning is important! \n",
    "        A previous iteration of this scoring pipeline said `Compare the following two dataframes to see if they are identical`.\n",
    "        That prompt turned out to be too stringent of criteria.\n",
    "        '''\n",
    "        system_prompt = \"Compare the following two dataframes. They are similar if they are almost identical, or if they convey the same information about the nba_roster dataset\"\n",
    "        system_prompt += \"Respond with valid JSON {'explanation' : str, 'similar' : bool}\"\n",
    "        user_prompt = (\n",
    "            f\"========== Dataframe 1 =========\\n{str(obj.data.get('df','None')).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += (\n",
    "            f\"========== Dataframe 2 =========\\n{str(obj.data['reference_df']).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += f\"Can you tell me if these dataframes are similar?\"\n",
    "        return {\n",
    "            \"system\": system_prompt,\n",
    "            \"user\": user_prompt\n",
    "        }\n",
    "    \n",
    "async def run_eval(dataset, args):\n",
    "\n",
    "    results = await run_evaluation_pipeline(dataset, args)\n",
    "\n",
    "    print(\"Total results:\", len(results))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "async def run_evaluation_pipeline(dataset, args):\n",
    "    results = EvaluationPipeline(args).call(dataset)\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    pbar = tqdm(desc=\"Saving results\", unit=\" results\")\n",
    "    async for result in results:\n",
    "        result_list.append(result)\n",
    "        pbar.update()\n",
    "    return result_list\n",
    "\n",
    "\n",
    "class EvaluationPipeline(GenerationPipeline):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.query_stage = QueryStage(args.sql_model_name)\n",
    "        self.score_stage = ScoreStage()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.query_stage(x)\n",
    "        x = self.score_stage(x)\n",
    "        return x\n",
    "    \n",
    "def load_gold_dataset(args):\n",
    "    path = f\"data/{args.gold_file_name}\"\n",
    "\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for index, obj in enumerate(reversed(list(reader))):\n",
    "            if index >= args.max_examples:\n",
    "                break\n",
    "            yield PromptObject(prompt=\"\", data=obj)\n",
    "\n",
    "def save_eval_results(results, args):\n",
    "    base_path = \"./data/results\"\n",
    "    now = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    experiment_name = f\"nba_sql_pipeline_{now}\"\n",
    "    experiment_dir = os.path.join(base_path, experiment_name)\n",
    "    os.makedirs(os.path.join(base_path, experiment_name))\n",
    "\n",
    "    # Write args to file\n",
    "    args_file_name = f\"{experiment_dir}/args.txt\"\n",
    "    with open(args_file_name, \"w\") as writer:\n",
    "        pprint(args.__dict__, writer)\n",
    "\n",
    "\n",
    "    def is_correct(r):\n",
    "        if (\n",
    "            (result.data[\"query_succeeded\"] and result.data['is_matching']) or \n",
    "            result.data[\"generated_query\"] == result.data['sql']\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Write sql results and errors to file\n",
    "    results_file_name = f\"{experiment_dir}/sql_results.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if not is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"reference_sql\": result.data['sql'],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    results_file_name = f\"{experiment_dir}/sql_errors.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Write statistics to file\n",
    "    average_sql_succeeded = sum(\n",
    "        [result.data[\"query_succeeded\"] for result in results]\n",
    "    ) / len(results)\n",
    "    average_correct = sum(\n",
    "        [result.data[\"query_succeeded\"] and result.data['is_matching'] for result in results]\n",
    "    ) / len(results)\n",
    "\n",
    "    file_name = f\"{experiment_dir}/summary.txt\"\n",
    "    with open(file_name, \"w\") as writer:\n",
    "        print(f\"Total size of eval dataset: {len(results)}\", file=writer)\n",
    "        print(f\"Total size of eval dataset: {len(results)}\")\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\", file=writer)\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\")\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\", file=writer)\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db445f8-77a5-4def-83b3-35746e08045c",
   "metadata": {},
   "source": [
    "Use pretrained model trained with the above dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce2757e1-7196-4b34-bdda-545a336a6300",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving results: 40 results [01:59,  2.99s/ results]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 40\n",
      "Total size of eval dataset: 40\n",
      "Percent Valid SQL Syntax: 100.0\n",
      "Percent Correct SQL Query: 92.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = Args(sql_model_name=\"3f7e740c0ea2227631a30d293b51564ad1b80727c3768a3b136fbae93170c1e2\", gold_file_name='gold-test-set-v2.jsonl')\n",
    "dataset = load_gold_dataset(args)\n",
    "results = await run_eval(dataset, args)\n",
    "save_eval_results(results, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
